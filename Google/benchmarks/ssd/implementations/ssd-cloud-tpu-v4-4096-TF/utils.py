# Copyright 2018 Google. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Utils for SSD train/eval with low level API."""

import sys

import tensorflow.compat.v1 as tf

from util import mllog

tf.flags.DEFINE_integer(
    'debug_print_op_level', 0,
    'Verbose level of add_print_op. Only levels <= this flag will be inserted.')

FLAGS = tf.flags.FLAGS

_mllogger = None
_weights_inited = set()


def get_mllogger() -> mllog.MLLogger:
  global _mllogger
  if not _mllogger:
    _mllogger = mllog.MLLogger()
  return _mllogger


def mllog_weight_init(tensor_name: str):
  """Weight initialization log function.

  Called by individual models to print weight tensors initialization logs.
  Keeps track of tensors in _weights_init_dict so theay are printed only once.

  Args:
    tensor_name: The name of the tensor according to MLPef rules.
  """
  mllogger = get_mllogger()
  if mllogger is not None and tensor_name not in _weights_inited:
    mllogger.event('weights_initialization', metadata={'tensor': tensor_name})
    _weights_inited.add(tensor_name)
  elif not mllogger:
    print('mllogger is None.')


def wrap_computation_in_while_loop(op_fn, n, host_name):
  """Wraps the ops generated by `op_fn` in tf.while_loop."""

  def computation(i):
    ops = op_fn()
    if not isinstance(ops, list):
      ops = [ops]
    with tf.control_dependencies(ops):
      return i + 1

  with tf.device(device_for_host(host_name)):
    return tf.while_loop(
        lambda i: tf.less(i, n),
        computation, [tf.constant(0)],
        parallel_iterations=1)


def device_for_host(host_name):
  return host_name + '/device:CPU:0'


def device_for_tpu_core(host_name, core=0):
  return host_name + '/device:TPU_REPLICATED_CORE:%d' % core


def tpu_ordinal_fn(shard_index_in_host):
  """Return the TPU ordinal associated with a shard."""
  return shard_index_in_host % FLAGS.num_shards_per_host


class InputDimsFlattener(object):
  """"Flatten input_partition_dims for spatial partition."""

  def __init__(self, input_partition_dims):
    self._initialized = False
    self._flattened_input_dims = None

    # This should have been validated in TPUConfig.
    assert len(input_partition_dims) <= 2, 'must have 1 or 2 elements.'
    if len(input_partition_dims) == 2:
      self._feature_dims, self._label_dims = input_partition_dims
    else:
      self._feature_dims = input_partition_dims[0]
      self._label_dims = None

    assert self._feature_dims is not None, ('input_partition_dims[0] must not '
                                            'be None')

  @property
  def flattened_input_dims(self):
    assert self._initialized, 'InputsStructureRecorder is not initialized.'
    return self._flattened_input_dims

  def validate_and_flatten_input_dims(self, features, labels):
    """Flatten input dims with the same order as flattened input tensors."""

    def _extract_key_names(tensor_or_dict):
      if isinstance(tensor_or_dict, dict):
        return sorted(tensor_or_dict.keys())
      return []

    if self._initialized:
      return self._flattened_input_dims

    has_labels = labels is not None
    feature_names = _extract_key_names(features)
    label_names = _extract_key_names(labels)
    feature_dims_names = _extract_key_names(self._feature_dims)
    if feature_dims_names != feature_names:
      raise ValueError('TPUConfig.input_partition_dims[0] mismatched feature'
                       ' keys. Expected {}, got {}'.format(
                           feature_names, feature_dims_names))

    label_dims_names = _extract_key_names(self._label_dims)
    if self._label_dims is not None and label_dims_names != label_names:
      raise ValueError('TPUConfig.input_partition_dims[1] mismatched label'
                       ' keys. Expected {}, got {}'.format(
                           label_names, label_dims_names))

    flattened_input_dims = []
    if feature_dims_names:
      # We need a fixed ordering for matching the tensors in features.
      flattened_input_dims.extend(
          [self._feature_dims[name] for name in feature_dims_names])
    else:
      flattened_input_dims.append(self._feature_dims)

    if label_dims_names:
      # We need a fixed ordering for matching the tensors in labels.
      flattened_input_dims.extend(
          [self._label_dims[name] for name in label_dims_names])
    else:
      if label_names:
        num_tensors_in_label = len(label_names)
      else:
        num_tensors_in_label = int(has_labels)
      # Setting `None` in input_partition_dims[1] will apply `None` to
      # all the tensors in labels, regardless of internal structure.
      flattened_input_dims.extend([self._label_dims] * num_tensors_in_label)

    self._flattened_input_dims = flattened_input_dims
    self._initialized = True


def fixed_padding(inputs: tf.Tensor,
                  kernel_size: int,
                  data_format: str = 'channels_last',
                  constant_values: float = 0.) -> tf.Tensor:
  """Pads the input along the spatial dimensions independently of input size.

  Args:
    inputs: `Tensor` of size `[batch, channels, height, width]` or `[batch,
      height, width, channels]` depending on `data_format`.
    kernel_size: `int` kernel size to be used for `conv2d` or max_pool2d`
      operations. Should be a positive integer.
    data_format: `str` either "channels_first" for `[batch, channels, height,
      width]` or "channels_last for `[batch, height, width, channels]`.
    constant_values: Passed to tf.pad().

  Returns:
    A padded `Tensor` of the same `data_format` with size either intact
    (if `kernel_size == 1`) or padded (if `kernel_size > 1`).
  """
  pad_total = kernel_size - 1
  pad_beg = pad_total // 2
  pad_end = pad_total - pad_beg
  if data_format == 'channels_first':
    padded_inputs = tf.pad(
        inputs, [[0, 0], [0, 0], [pad_beg, pad_end], [pad_beg, pad_end]],
        constant_values=constant_values)
  else:
    padded_inputs = tf.pad(
        inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]],
        constant_values=constant_values)

  return padded_inputs


def conv2d_fixed_padding(inputs: tf.Tensor,
                         filters: int,
                         kernel_size: int,
                         strides: int,
                         data_format: str = 'channels_last',
                         **kwargs) -> tf.Tensor:
  """Strided 2-D convolution with explicit padding.

  The padding is consistent and is based only on `kernel_size`, not on the
  dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).

  This function can be used to mimic PyTorch's symmetric padding. For the
  discrepancy of padding between TF and PyTorch, see
  https://www.tensorflow.org/api_docs/python/tf/nn#notes_on_padding_2

  Args:
    inputs: `Tensor` of size `[batch, channels, height_in, width_in]`.
    filters: `int` number of filters in the convolution.
    kernel_size: `int` size of the kernel to be used in the convolution.
    strides: `int` strides of the convolution.
    data_format: `str` either "channels_first" for `[batch, channels, height,
      width]` or "channels_last for `[batch, height, width, channels]`.
    **kwargs: Other params passed to `tf.layers.conv2d`

  Returns:
    A `Tensor` of shape `[batch, filters, height_out, width_out]`.
  """
  if strides > 1:
    inputs = fixed_padding(inputs, kernel_size, data_format=data_format)

  return tf.layers.conv2d(
      inputs=inputs,
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=('SAME' if strides == 1 else 'VALID'),
      data_format=data_format,
      **kwargs)


def add_print_op(x: tf.Tensor,
                 name: str,
                 level: int,
                 force: bool = False,
                 **kwargs) -> tf.Tensor:
  """Add a print op of tensor for debugging purpose.

  Args:
    x: The tensor to print.
    name: Name of tensor.
    level: Verbose level.
    force: The print op may be evaded if evaluation of x is evaded by symbolic
      derivative (b/71895483) . To force print, we will further add the print op
      to tf.GraphKeys.UPDATE_OPS required by the optimizer.
    **kwargs: Passed to `tf.print`

  Returns:
    A copy of x.
  """
  if level <= FLAGS.debug_print_op_level:
    print_op = tf.tpu.outside_compilation(
        lambda y: tf.print(  # pylint: disable=g-long-lambda
            f'>>>{name} {y.dtype} {y.shape} >>> ',
            y,
            output_stream=sys.stderr,
            **kwargs), x)
    with tf.control_dependencies([print_op]):
      x = tf.identity(x)
    if force:
      tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, print_op)
    return x
  else:
    return x
