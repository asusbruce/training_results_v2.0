#!/bin/bash
DATESTAMP=`date +'%y%m%d%H%M%S'`
export INPUT_FILES_DIR_UNPACKED=/root/datasets/bert_pretraining/training
export INPUT_FILES_DIR_PACKED=/root/datasets/bert_pretraining/packed
export EVAL_FILES_DIR=/root/datasets/bert_pretraining/evaluation
export OUTPUT_DIR=/tmp/bert_pretrain/phase_2
export LOG_DIR=/tmp/bert_pretrain/phase_2
export INITIAL_CHECKPOINT=/root/datasets/bert_pretraining/MLPerf_BERT_checkpoint/model.ckpt-28252
export BERT_CONFIG_DIR=/root/datasets/bert_pretraining/bert_config
export TRAIN_BATCH_SIZE=12
export EVAL_BATCH_SIZE=80
export MAX_EVAL_STEPS=1
export NUM_DIST_EVAL_WORKERS=125
export TRAIN_STEPS=1140
export WARMUP_STEPS=0
export LEARNING_RATE=0.0025
export LAMB_BETA_1=0.66
export LAMB_BETA_2=0.998
export EPSILON=1e-06
export LAMB_WEIGHT_DECAY_RATE=0.01
export LAMB_LEARNING_RATE_DECAY_POLY_POWER=1.0
export NUM_ACCUMULATION_STEPS=2
export SAMPLES_START_EVAL=0
export SAVE_CHECKPOINTS_STEPS=57
export PACKED_DATA=True
export USE_HOROVOD=True
export HLS_TYPE="HLS1-H"
export NUM_WORKERS_TOTAL=128
export RUN_TPC_FUSER=True
export MPI_TCP_INCLUDE=enp24s0f0
export TF_CPU_RUNTIME_FALLBACK=forbid
export TF_HCCL_MEMORY_ALLOWANCE_MB=1536
export HABANA_INITIAL_WORKSPACE_SIZE_MB=4600
export CPU_BIND_TYPE=numa
export USE_LIGHTWEIGHT_CHECKPOINT=True
export DO_TRAIN=True
export DO_EVAL=True
export USE_ASYNC_CHECKPOINTING=True
export EXPERIMENTAL_SLACK=False
