# Copyright (c) 2019 Graphcore Ltd. All rights reserved.
import time
import numba as nb
from numba.typed import List
import numpy as np
import random
import glob
import os
from tqdm import tqdm
from logging import getLogger
from functools import reduce
from concurrent.futures import ThreadPoolExecutor

from .dataset import DataSet

logger = getLogger(__name__)

# The contents of a packed pretraining dataset:
# Each example contains the following arrays (in this order and shape)
#   SEQ:
#   packed_input_ids, [sequence_length]
#   packed_input_mask, [sequence_length]
#   packed_segment_ids, [sequence_length]
#   packed_positions, [sequence_length]
#   MLM:
#   packed_masked_lm_ids, [mask_tokens + max_sequences_per_pack - 1]
#   packed_masked_lm_weights, [mask_tokens + max_sequences_per_pack -1]
#   NSP:
#   packed_next_sentence_labels, [max_sequences_per_pack]
#   packed_next_sentence_weights, [max_sequences_per_pack]


def data_file_format(sequence_length, mask_tokens, max_sequences_per_pack):
    return [sequence_length,
            sequence_length,
            sequence_length,
            sequence_length,
            mask_tokens + max_sequences_per_pack - 1,
            mask_tokens + max_sequences_per_pack - 1,
            max_sequences_per_pack,
            max_sequences_per_pack]


def data_ranges(sequence_length, mask_tokens, vocab_length, max_sequences_per_pack):
    return [vocab_length,
            max_sequences_per_pack + 1,
            2,
            sequence_length,
            vocab_length,
            max_sequences_per_pack + 1,
            2,
            max_sequences_per_pack + 1]


class BinaryDataLoader(object):
    '''
    Iterates binary input files into list of N np.ndarrays with shapes (batch_size, samples_sizes[i]) for i in N

    :param input_files: Iterable of paths to binary files generated by create_pretraining_data.py
    :param sample_size: Iterable of the sizes of each element in the binary file. See data_file_format for the default
    :param batch_size: Number of samples to return each iteration
    :param dtype: Numpy type of binary files
    :param shuffle: If True, shuffle the input_files and the data contained.
    :param duplication_factor:
        The number of times each file contains the same sample. This will then only take 1/duplication_factor
        from each file before moving to the next.
    :param synthetic: If True, generate random data instead of reading from input_files
    '''

    def __init__(self,
                 input_files,
                 sample_sizes,
                 batch_size=1,
                 dtype=np.int32,
                 shuffle=True,
                 seed=1984,
                 duplication_factor=1,
                 start_data_at_epoch=0,
                 drop_remainder=True,
                 popdist_size=1,
                 popdist_rank=0,
                 use_popdist=False):
        self.files = []
        self.input_patterns = input_files
        for pattern in input_files:
            self.files.extend(glob.glob(pattern))
        self.files = sorted(self.files)
        if len(self.files) == 0:
            raise RuntimeError(f"No files found matching glob patterns: {input_files}")
        self.sample_size = reduce(lambda a, s: a + s, sample_sizes, 0)
        self.sample_sizes = sample_sizes
        self.batch_size = batch_size
        self.dtype = dtype
        self.file_index = 0
        self.data_index = 0
        self.use_popdist = False
        self.popdist_size = popdist_size
        self.popdist_rank = popdist_rank
        self.file_duplication_index = [start_data_at_epoch % duplication_factor] * len(self.files)
        self.duplication_factor = duplication_factor
        self.shuffle = shuffle
        self._rng = np.random.default_rng(seed)
        self.drop_remainder = drop_remainder
        self.len = None

    def samples_in_file(self, filename):
        bytes_per_sample = self.sample_size * self.dtype().itemsize
        num_bytes = os.path.getsize(filename)
        if (num_bytes % bytes_per_sample) != 0:
            raise RuntimeError(f"Input file: {filename} does not align to the size of a sample. Check the dataset was generated correctly")
        duplicated_samples = num_bytes // bytes_per_sample
        return duplicated_samples // self.duplication_factor

    def __len__(self):
        if self.len is None:
            total_bytes = reduce(lambda a, f: a + self.samples_in_file(f), self.files, 0)
            self.len = total_bytes // (self.batch_size)
        return self.len

    def __iter__(self):
        self.file_index = 0
        self.data_index = 0
        if self.shuffle:
            self._rng.shuffle(self.files)
        self.load_data()
        return self

    def __next__(self):
        data = self.get_data(self.batch_size)
        # Split the batch into separate np.ndarrays
        items = []
        total = 0
        for size in self.sample_sizes:
            items.append(np.array(data[:, total:total + size]))
            total += size
        return items

    def get_data(self, batch_size):
        """
        Slice batch_size samples from self.data or from the next file if there is not enough left
        """
        if self.data_index + batch_size > self.data.shape[0]:
            prev_data = self.data[self.data_index:, :]
            still_required = batch_size - prev_data.shape[0]
            self.load_data()
            next_data = self.get_data(still_required)
            data = np.concatenate((prev_data, next_data), axis=0)
        else:
            data = self.data[self.data_index:self.data_index + batch_size, :]
            self.data_index += batch_size
        return data

    def load_data(self):
        # This drops the remainder
        if self.file_index >= len(self.files):
            raise StopIteration
        self.data = self.load_file()

        if self.shuffle:
            self._rng.shuffle(self.files)

    def load_file(self):
        new_data = self.load_file_index(self.file_index)
        self.file_index += 1
        self.data_index = 0
        return new_data

    def load_file_index(self, file_index):
        filename = self.files[file_index]
        # Input files are assumed to be duplicated by create_pretraining_data only within a single file.
        # So for preprocessed files: A, B, C. The output files are created: AAA.., BBB.., CCC..
        # This makes sure in a single epoch A, B & C are all used once.
        count = self.samples_in_file(filename) * self.sample_size
        offset_bytes = count * self.file_duplication_index[file_index] * self.dtype().itemsize

        new_data = np.fromfile(filename, self.dtype, count=count, offset=offset_bytes)
        new_data = new_data.reshape(new_data.size // self.sample_size,
                                    self.sample_size)

        self.file_duplication_index[file_index] = \
            (self.file_duplication_index[file_index] + 1) % self.duplication_factor

        return new_data


class CachedDataLoader(BinaryDataLoader):
    """
    Same as the BinaryDataLoader but preloads the specified number of epochs into memory ahead of time.
    :param epochs_to_cache:
        Specify the number of epochs to keep loaded in memory. This can reduce the number of times the inputs
        are read. It is recommened to make this as large as possible as the dataset files can be very large due to duplication factor.
        Must be greater than 0.
    """

    def __init__(self,
                 *args,
                 epochs_to_cache=1,
                 popdist_size=1,
                 popdist_rank=0,
                 use_popdist=False,
                 **kwargs):
        super().__init__(*args, **kwargs)
        self.duplication_factor = kwargs["duplication_factor"]
        self.epochs_to_cache = epochs_to_cache
        self.data_cache = []
        self.cache_index = 0
        self.use_popdist = use_popdist
        self.popdist_size = popdist_size
        self.popdist_rank = popdist_rank

        if self.epochs_to_cache < 1:
            raise RuntimeError("epochs_to_cache must be greater than 0")

        # Should not cache more epochs than duplication factor
        if self.epochs_to_cache > self.duplication_factor:
            self.epochs_to_cache = self.duplication_factor

        self.load_cache()
        need_extra_batch = not self.drop_remainder and (self.data_cache[0].shape[0] % self.batch_size != 0)
        self.len = self.data_cache[0].shape[0] // self.batch_size + int(need_extra_batch)
        self.num_padding_samples = int(need_extra_batch)*(self.batch_size - self.data_cache[0].shape[0] % self.batch_size)

    def get_data(self, batch_size):
        if self.data_index + batch_size > self.data.shape[0]:
            # Handle remainder of dataset without dropping it
            if self.data_index < self.len * self.batch_size:
                # To prevent nan's the padding cannot be all zero.
                padding = np.broadcast_to(self.data[0], [self.num_padding_samples, self.data.shape[1]])
                self.data = np.concatenate([self.data, padding])
            else:
                raise StopIteration

        data = self.data[self.data_index:self.data_index + batch_size, :]
        self.data_index += batch_size
        return data

    def load_data(self):
        if self.cache_index >= len(self.data_cache):
            # If all duplications have already been loaded, then just go back
            # to the original
            if len(self.data_cache) == self.duplication_factor:
                self.cache_index = 0
            # Otherwise load new data
            else:
                self.load_cache()
        self.data = self.data_cache[self.cache_index]
        self.cache_index += 1

    def load_cache(self):
        self.cache_index = 0
        self.data_cache = []
        logger.info("Filling Dataset Cache")

        start_time = time.time()
        for __ in range(self.epochs_to_cache):
            data = List()
            # since it is disk reading hyper threading should not help
            num_workers = os.cpu_count() / 2
            with ThreadPoolExecutor(max_workers=num_workers) as executor:
                future = executor.map(self.load_file_index, range(len(self.files)))
            for res in future:
                data.append(res)

            # Concatenate and shuffle the data that has been read in
            @nb.jit(nopython=True, parallel=True, cache=True)
            def concat_shuffle(buffer, data, indices, samples_per_file):
                cumulative_samples_per_epoch = np.zeros((len(samples_per_file)), dtype=nb.int32)
                cumulative_samples_per_epoch[1:] = np.cumsum(samples_per_file)[:-1]

                for i in nb.prange(len(data)):
                    start = cumulative_samples_per_epoch[i]
                    end = cumulative_samples_per_epoch[i] + samples_per_file[i]
                    access_pattern = indices[start:end]
                    for k, j in enumerate(access_pattern):
                        buffer[j] = data[i][k]
                return buffer

            # Create an array to hold the samples
            samples_per_file = np.array([self.samples_in_file(f) for f in self.files])
            total_samples_per_epoch = int(sum(samples_per_file))
            buffer = np.empty([total_samples_per_epoch, self.sample_size], self.dtype)

            # Shuffle the access indices instead of shuffling the whole dataset
            indices = np.arange(total_samples_per_epoch)
            if self.shuffle:
                self._rng.shuffle(indices)

            # Concatenate and shuffle the data from each file
            # using the compiled Numba function
            data = concat_shuffle(buffer, data, indices, samples_per_file)

            # Handle distributed data split
            if self.use_popdist:
                data = data[self.popdist_rank:][::self.popdist_size]

            self.data_cache.append(data)
            self.file_index = 0
        logger.info(f"Loading {self.input_patterns} from disk took: {time.time() - start_time:3.1f} seconds.")


class GeneratedDataLoader(BinaryDataLoader):
    """
    Same as the BinaryDataLoader but generates random data instead of reading from input_files
    :param generated_ranges: Iterable of the max value each element of a sample can be. See data_ranges for the default
    """

    def __init__(self,
                 *args,
                 length=1,
                 generated_ranges=None,
                 **kwargs):
        super().__init__(*args, **kwargs)
        self.generated_ranges = generated_ranges
        self.len = length

        if self.generated_ranges is None:
            raise RuntimeError("keyword argument 'generated_ranges' must not be None")

    def __iter__(self):
        self.data_index = 0
        return self

    def __next__(self):
        if self.data_index >= self.len:
            raise StopIteration
        items = []
        for size, max_value in zip(self.sample_sizes, self.generated_ranges):
            items.append(np.random.randint(0, max_value, [self.batch_size, size]))
        self.data_index += 1
        return items


class BertDataTransform(object):
    '''
    Masks the indices that are larger than the vocab_length
    '''

    def __init__(self, dataloader, vocab_length, mask_tokens):
        self.dataloader = dataloader
        self.vocab_length = vocab_length
        self.mask_tokens = mask_tokens

    def __len__(self):
        return len(self.dataloader)

    def __iter__(self):
        self.dataloader_iterator = iter(self.dataloader)
        return self

    def __next__(self):
        items = next(self.dataloader_iterator)
        # The vocab_length may be smaller than the original vocab so
        # Mask values that are not within the vocab_length
        # 100 is unknown token [UNK]
        # 0 in the label is padding
        OOB = items[0] >= self.vocab_length
        items[0][OOB] = 100

        # TODO: If Ind == [MASK] and label > vocab_length, should [MASK] be changed to [UNK]
        OOB = items[5] >= self.vocab_length
        items[5][OOB] = 0

        # Force use of uint32 for all inputs.
        for i in range(len(items)):
            items[i] = items[i].astype(np.uint32)
        return items


def get_bert_dataset(tensor_shapes,
                     input_files,
                     seed,
                     sequence_length,
                     mask_tokens,
                     max_sequences_per_pack,
                     vocab_length,
                     batch_size,
                     batches_per_step,
                     replication_factor=1,
                     accumulation_factor=1,
                     duplication_factor=1,
                     shuffle=True,
                     generated_data=False,
                     epochs_to_cache=0,
                     drop_remainder=True,
                     continue_training_from_epoch=0,
                     use_popdist=False,
                     popdist_size=1,
                     popdist_rank=0):
    samples_per_step = batch_size * batches_per_step * replication_factor * accumulation_factor
    if len(input_files) == 0 and not generated_data:
        raise ValueError("No input files were provided for the BERT dataset.")
    data_loader_args = dict(
        input_files=input_files,
        sample_sizes=data_file_format(sequence_length, mask_tokens, max_sequences_per_pack),
        batch_size=samples_per_step,
        duplication_factor=duplication_factor,
        start_data_at_epoch=continue_training_from_epoch,
        shuffle=shuffle,
        drop_remainder=drop_remainder,
        seed=seed,
        popdist_size=popdist_size,
        popdist_rank=popdist_rank,
        use_popdist=use_popdist
    )
    if generated_data:
        length = 10
        if use_popdist:
            length = popdist_size
        dl = GeneratedDataLoader(**data_loader_args,
                                 length=length,
                                 generated_ranges=data_ranges(sequence_length, mask_tokens, vocab_length, max_sequences_per_pack))
    else:
        dl = CachedDataLoader(**data_loader_args,
                              epochs_to_cache=epochs_to_cache)

    if len(dl) == 0:
        raise ValueError("Insufficient data for training parameters.")

    bert_ds = BertDataTransform(dl, vocab_length, mask_tokens)
    ds = DataSet(bert_ds,
                 tensor_shapes,
                 batches_per_step=batches_per_step,
                 replication_factor=replication_factor,
                 accumulation_factor=accumulation_factor)

    return ds
