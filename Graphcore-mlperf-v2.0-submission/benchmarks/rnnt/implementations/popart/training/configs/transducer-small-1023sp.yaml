# Copyright (c) 2021 Graphcore Ltd. All rights reserved.
# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# This file has been modified by Graphcore Ltd.

tokenizer:
    # this path should be relative to parent of Librispeech dataset
    sentpiece_model: sentencepieces/librispeech1023.model
    labels: [" ", "a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m",
             "n", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z", "'"]

input_val:
  audio_dataset: &val_dataset
    sample_rate: &sample_rate 16000
    trim_silence: true
    normalize_transcripts: true
    # this is slightly more than the max-duration of clips in librispeech-dev-clean 
    max_duration: 32.7

  filterbank_features: &val_features
    normalize: per_feature
    sample_rate: *sample_rate
    window_size: 0.02
    window_stride: 0.01
    window: hann
    n_fft: 512
    n_filt: &n_filt 80
    dither: 0.00001
  frame_splicing: &val_splicing
    frame_stacking: 3
    frame_subsampling: 3

# For training we keep samples < 16.7s and apply augmentation
input_train:
  audio_dataset:
    <<: *val_dataset
    max_duration: 16.7
    speed_perturbation:
        min_rate: 0.85
        max_rate: 1.15
        p: 1.0

  filterbank_features: *val_features
  frame_splicing: *val_splicing

  spec_augment:
    freq_masks: 2
    min_freq: 0
    max_freq: 20
    time_masks: 10
    min_time: 0
    max_time: 0.03
    
# Used for reference model
rnnt:
  in_feats: &in_feats 240 # n_filt x frame_stacking

  enc_n_hid: 1024
  enc_pre_rnn_layers: 2
  enc_post_rnn_layers: 3
  enc_stack_time_factor: 2
  enc_dropout: 0.1

  pred_n_hid: &pred_n_hid 512
  pred_rnn_layers: &pred_rnn_layers 1
  pred_dropout: &pred_dropout 0.3

  joint_n_hid: &joint_n_hid 512
  joint_dropout: &joint_dropout 0.3

  forget_gate_bias: &forget_gate_bias 1.0
  weights_init_scale: &weights_init_scale 0.5


transformer_transducer:
  in_feats: *in_feats  # n_filt x frame_stacking

  subsampling_factor: 2
  num_encoder_layers: 12
  encoder_dim: 256
  num_attention_heads: 8
  enc_dropout: 0.1
  kernel_size: 16

  pred_n_hid: *pred_n_hid
  pred_rnn_layers: *pred_rnn_layers
  pred_dropout: *pred_dropout

  joint_n_hid: *joint_n_hid
  joint_dropout: *joint_dropout

  forget_gate_bias: *forget_gate_bias
  weights_init_scale: *weights_init_scale
